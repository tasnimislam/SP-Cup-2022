{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a6aebd8",
   "metadata": {
    "papermill": {
     "duration": 0.036661,
     "end_time": "2022-03-29T12:20:16.730908",
     "exception": false,
     "start_time": "2022-03-29T12:20:16.694247",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This is our implementation of resnet inspired model on raw wave dataset(time domain) dataset. Here one can just hit run all and results will be generated in 2 csv files as inteded. The weights and models will be also saved for reproducibility. Thus this notebook can be used to train custom version of our model and experiment further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b7a926",
   "metadata": {
    "papermill": {
     "duration": 0.034997,
     "end_time": "2022-03-29T12:20:16.801627",
     "exception": false,
     "start_time": "2022-03-29T12:20:16.766630",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 0. Setup\n",
    "\n",
    "At first we are importing and setting up all the libraries to implement this solution. If some of the files are not installed in your device, please refer to the readme file for installation direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ccc28f",
   "metadata": {
    "papermill": {
     "duration": 0.034981,
     "end_time": "2022-03-29T12:20:16.873522",
     "exception": false,
     "start_time": "2022-03-29T12:20:16.838541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## utility libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f70c8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: notebook>=6.3.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from -r requirement (1).txt (line 1)) (6.4.8)\n",
      "Requirement already satisfied: tqdm>=4.60.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from -r requirement (1).txt (line 2)) (4.62.0)\n",
      "Requirement already satisfied: pandas>=1.2.4 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from -r requirement (1).txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.19.2 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from -r requirement (1).txt (line 4)) (1.21.5)\n",
      "Requirement already satisfied: ast-tools>=0.1.4 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from -r requirement (1).txt (line 5)) (0.1.8)\n",
      "Requirement already satisfied: scikit-learn>=0.24.2 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from -r requirement (1).txt (line 6)) (0.24.2)\n",
      "Requirement already satisfied: scipy>=1.6.3 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from -r requirement (1).txt (line 7)) (1.7.1)\n",
      "Requirement already satisfied: matplotlib>=3.4.1 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from -r requirement (1).txt (line 8)) (3.4.2)\n",
      "Requirement already satisfied: scikit-image>=0.18.1 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from -r requirement (1).txt (line 9)) (0.18.2)\n",
      "Requirement already satisfied: tensorflow-gpu>=2.4.1 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from -r requirement (1).txt (line 11)) (2.8.0)\n",
      "Requirement already satisfied: keras>=2.4.3 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from -r requirement (1).txt (line 12)) (2.8.0)\n",
      "Requirement already satisfied: librosa==0.9.1 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from -r requirement (1).txt (line 14)) (0.9.1)\n",
      "Requirement already satisfied: audioread>=2.1.5 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from librosa==0.9.1->-r requirement (1).txt (line 14)) (2.1.9)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from librosa==0.9.1->-r requirement (1).txt (line 14)) (0.10.3.post1)\n",
      "Requirement already satisfied: decorator>=4.0.10 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from librosa==0.9.1->-r requirement (1).txt (line 14)) (5.0.9)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from librosa==0.9.1->-r requirement (1).txt (line 14)) (1.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from librosa==0.9.1->-r requirement (1).txt (line 14)) (21.0)\n",
      "Requirement already satisfied: numba>=0.45.1 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from librosa==0.9.1->-r requirement (1).txt (line 14)) (0.55.1)\n",
      "Requirement already satisfied: resampy>=0.2.2 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from librosa==0.9.1->-r requirement (1).txt (line 14)) (0.2.2)\n",
      "Requirement already satisfied: pooch>=1.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from librosa==0.9.1->-r requirement (1).txt (line 14)) (1.6.0)\n",
      "Requirement already satisfied: traitlets>=4.2.1 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from notebook>=6.3.0->-r requirement (1).txt (line 1)) (5.0.5)\n",
      "Requirement already satisfied: jupyter-core>=4.6.1 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from notebook>=6.3.0->-r requirement (1).txt (line 1)) (4.7.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from notebook>=6.3.0->-r requirement (1).txt (line 1)) (3.0.3)\n",
      "Requirement already satisfied: nbformat in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from notebook>=6.3.0->-r requirement (1).txt (line 1)) (5.1.3)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from notebook>=6.3.0->-r requirement (1).txt (line 1)) (0.13.1)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from notebook>=6.3.0->-r requirement (1).txt (line 1)) (6.4.2)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from notebook>=6.3.0->-r requirement (1).txt (line 1)) (20.0.0)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from notebook>=6.3.0->-r requirement (1).txt (line 1)) (21.3.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from notebook>=6.3.0->-r requirement (1).txt (line 1)) (1.5.4)\n",
      "Requirement already satisfied: jupyter-client>=5.3.4 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from notebook>=6.3.0->-r requirement (1).txt (line 1)) (6.1.12)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from notebook>=6.3.0->-r requirement (1).txt (line 1)) (6.1)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from notebook>=6.3.0->-r requirement (1).txt (line 1)) (5.3.4)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from notebook>=6.3.0->-r requirement (1).txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from notebook>=6.3.0->-r requirement (1).txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from notebook>=6.3.0->-r requirement (1).txt (line 1)) (0.13.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from tqdm>=4.60.0->-r requirement (1).txt (line 2)) (0.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from pandas>=1.2.4->-r requirement (1).txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from pandas>=1.2.4->-r requirement (1).txt (line 3)) (2021.1)\n",
      "Requirement already satisfied: astor in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from ast-tools>=0.1.4->-r requirement (1).txt (line 5)) (0.8.1)\n",
      "Requirement already satisfied: libcst in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from ast-tools>=0.1.4->-r requirement (1).txt (line 5)) (0.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from scikit-learn>=0.24.2->-r requirement (1).txt (line 6)) (2.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from matplotlib>=3.4.1->-r requirement (1).txt (line 8)) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from matplotlib>=3.4.1->-r requirement (1).txt (line 8)) (8.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from matplotlib>=3.4.1->-r requirement (1).txt (line 8)) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from matplotlib>=3.4.1->-r requirement (1).txt (line 8)) (2.4.7)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from scikit-image>=0.18.1->-r requirement (1).txt (line 9)) (2021.7.30)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from scikit-image>=0.18.1->-r requirement (1).txt (line 9)) (1.1.1)\n",
      "Requirement already satisfied: imageio>=2.3.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from scikit-image>=0.18.1->-r requirement (1).txt (line 9)) (2.9.0)\n",
      "Requirement already satisfied: networkx>=2.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from scikit-image>=0.18.1->-r requirement (1).txt (line 9)) (2.6.2)\n",
      "Requirement already satisfied: gast>=0.2.1 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (0.4.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (1.12)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (2.8.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (1.39.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (4.1.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (1.1.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (52.0.0.post20210125)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (3.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (1.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (1.12.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (0.24.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (3.17.3)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (0.13.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (13.0.0)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (0.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (0.36.2)\n",
      "Requirement already satisfied: cached-property in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from h5py>=2.9.0->tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (1.5.2)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from jupyter-core>=4.6.1->notebook>=6.3.0->-r requirement (1).txt (line 1)) (228)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from numba>=0.45.1->librosa==0.9.1->-r requirement (1).txt (line 14)) (0.38.0)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from pooch>=1.0->librosa==0.9.1->-r requirement (1).txt (line 14)) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from pooch>=1.0->librosa==0.9.1->-r requirement (1).txt (line 14)) (2.26.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1->-r requirement (1).txt (line 14)) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1->-r requirement (1).txt (line 14)) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1->-r requirement (1).txt (line 14)) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.1->-r requirement (1).txt (line 14)) (2021.10.8)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from soundfile>=0.10.2->librosa==0.9.1->-r requirement (1).txt (line 14)) (1.14.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.9.1->-r requirement (1).txt (line 14)) (2.20)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (0.4.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (2.0.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (1.35.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (1.8.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (4.6.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (3.1.1)\n",
      "Requirement already satisfied: pywinpty>=1.1.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from terminado>=0.8.3->notebook>=6.3.0->-r requirement (1).txt (line 1)) (2.0.2)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from argon2-cffi->notebook>=6.3.0->-r requirement (1).txt (line 1)) (21.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow-gpu>=2.4.1->-r requirement (1).txt (line 11)) (3.5.0)\n",
      "Requirement already satisfied: ipython>=5.0.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from ipykernel->notebook>=6.3.0->-r requirement (1).txt (line 1)) (7.22.0)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from ipython>=5.0.0->ipykernel->notebook>=6.3.0->-r requirement (1).txt (line 1)) (0.7.5)\n",
      "Requirement already satisfied: backcall in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from ipython>=5.0.0->ipykernel->notebook>=6.3.0->-r requirement (1).txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from ipython>=5.0.0->ipykernel->notebook>=6.3.0->-r requirement (1).txt (line 1)) (0.17.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from ipython>=5.0.0->ipykernel->notebook>=6.3.0->-r requirement (1).txt (line 1)) (3.0.17)\n",
      "Requirement already satisfied: pygments in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from ipython>=5.0.0->ipykernel->notebook>=6.3.0->-r requirement (1).txt (line 1)) (2.9.0)\n",
      "Requirement already satisfied: parso>=0.7.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->notebook>=6.3.0->-r requirement (1).txt (line 1)) (0.8.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->notebook>=6.3.0->-r requirement (1).txt (line 1)) (0.2.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from jinja2->notebook>=6.3.0->-r requirement (1).txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.2 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from libcst->ast-tools>=0.1.4->-r requirement (1).txt (line 5)) (5.4.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from libcst->ast-tools>=0.1.4->-r requirement (1).txt (line 5)) (0.7.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from typing-inspect>=0.4.0->libcst->ast-tools>=0.1.4->-r requirement (1).txt (line 5)) (0.4.3)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from nbconvert->notebook>=6.3.0->-r requirement (1).txt (line 1)) (0.5.11)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from nbconvert->notebook>=6.3.0->-r requirement (1).txt (line 1)) (1.5.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from nbconvert->notebook>=6.3.0->-r requirement (1).txt (line 1)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from nbconvert->notebook>=6.3.0->-r requirement (1).txt (line 1)) (0.1.2)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from nbconvert->notebook>=6.3.0->-r requirement (1).txt (line 1)) (0.4)\n",
      "Requirement already satisfied: testpath in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from nbconvert->notebook>=6.3.0->-r requirement (1).txt (line 1)) (0.6.0)\n",
      "Requirement already satisfied: bleach in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from nbconvert->notebook>=6.3.0->-r requirement (1).txt (line 1)) (4.1.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from nbconvert->notebook>=6.3.0->-r requirement (1).txt (line 1)) (0.8.4)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from nbformat->notebook>=6.3.0->-r requirement (1).txt (line 1)) (4.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=6.3.0->-r requirement (1).txt (line 1)) (21.4.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=6.3.0->-r requirement (1).txt (line 1)) (5.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=6.3.0->-r requirement (1).txt (line 1)) (0.18.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\manami\\anaconda3\\envs\\newgpu\\lib\\site-packages (from bleach->nbconvert->notebook>=6.3.0->-r requirement (1).txt (line 1)) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r \"requirement (1).txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f45f4fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:20:16.958550Z",
     "iopub.status.busy": "2022-03-29T12:20:16.957685Z",
     "iopub.status.idle": "2022-03-29T12:20:23.872909Z",
     "shell.execute_reply": "2022-03-29T12:20:23.872326Z",
     "shell.execute_reply.started": "2022-03-29T09:23:06.663622Z"
    },
    "papermill": {
     "duration": 6.964026,
     "end_time": "2022-03-29T12:20:23.873053",
     "exception": false,
     "start_time": "2022-03-29T12:20:16.909027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from IPython import display\n",
    "from pathlib import Path\n",
    "from IPython.display import Audio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data handling libraries\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ML and DL Libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8e3919",
   "metadata": {
    "papermill": {
     "duration": 0.034783,
     "end_time": "2022-03-29T12:20:23.943205",
     "exception": false,
     "start_time": "2022-03-29T12:20:23.908422",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## declaring hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17a5ac87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:20:24.019446Z",
     "iopub.status.busy": "2022-03-29T12:20:24.017942Z",
     "iopub.status.idle": "2022-03-29T12:20:24.021357Z",
     "shell.execute_reply": "2022-03-29T12:20:24.021740Z",
     "shell.execute_reply.started": "2022-03-29T09:23:13.465645Z"
    },
    "papermill": {
     "duration": 0.042533,
     "end_time": "2022-03-29T12:20:24.021867",
     "exception": false,
     "start_time": "2022-03-29T12:20:23.979334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "SR = 16000 #sampling rate of each audio file\n",
    "BATCH_SIZE = 10\n",
    "AUD_LENGTH = 10# we are taking 10s audio for each sample to have an uniform dataset\n",
    "TRAIN_TEST_SPLIT = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b7a34b",
   "metadata": {
    "papermill": {
     "duration": 0.034734,
     "end_time": "2022-03-29T12:20:24.091458",
     "exception": false,
     "start_time": "2022-03-29T12:20:24.056724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## all file paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f944ef8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:20:24.166151Z",
     "iopub.status.busy": "2022-03-29T12:20:24.163899Z",
     "iopub.status.idle": "2022-03-29T12:20:24.168450Z",
     "shell.execute_reply": "2022-03-29T12:20:24.168000Z",
     "shell.execute_reply.started": "2022-03-29T09:23:13.471439Z"
    },
    "papermill": {
     "duration": 0.041716,
     "end_time": "2022-03-29T12:20:24.168560",
     "exception": false,
     "start_time": "2022-03-29T12:20:24.126844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# insert needed paths here\n",
    "\n",
    "# this is the known and unknown dataset path\n",
    "DATASET_AUDIO_PATH = 'classwise_dataset/'\n",
    "\n",
    "# this is the random extra data addition folders\n",
    "ASVSPOOF_DATA_PATH  = 'external files/asvspoof'\n",
    "LIBRISPEECH_DATA_PATH = 'external files/librispeech'\n",
    "\n",
    "# this is the evaluation folder for phase 1 and 2\n",
    "EVAL_PATH_1 = 'spcup_2022_eval_part1'\n",
    "EVAL_PATH_2 = 'spcup_2022_eval_part1'\n",
    "\n",
    "### saving paths\n",
    "CSV_DIR = './'\n",
    "MODEL_SAVE_DIR = './'\n",
    "WEIGHT_SAVE_DIR = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e48df1",
   "metadata": {
    "papermill": {
     "duration": 0.034723,
     "end_time": "2022-03-29T12:20:24.238171",
     "exception": false,
     "start_time": "2022-03-29T12:20:24.203448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Generating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2da6afb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:20:24.314459Z",
     "iopub.status.busy": "2022-03-29T12:20:24.313874Z",
     "iopub.status.idle": "2022-03-29T12:20:24.938490Z",
     "shell.execute_reply": "2022-03-29T12:20:24.938871Z",
     "shell.execute_reply.started": "2022-03-29T09:23:13.491515Z"
    },
    "papermill": {
     "duration": 0.666055,
     "end_time": "2022-03-29T12:20:24.939016",
     "exception": false,
     "start_time": "2022-03-29T12:20:24.272961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our class names: ['0', '1', '2', '3', '4', '5']\n",
      "Processing speaker 0\n",
      "Actual Label  0\n",
      "Processing speaker 1\n",
      "Actual Label  1\n",
      "Processing speaker 2\n",
      "Actual Label  2\n",
      "Processing speaker 3\n",
      "Actual Label  3\n",
      "Processing speaker 4\n",
      "Actual Label  4\n",
      "Processing speaker 5\n",
      "Actual Label  5\n",
      "Found 6000 files belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# getting audio dataset path to divide into 3 datasets and also for making tf datasets later\n",
    "\n",
    "class_names = os.listdir(DATASET_AUDIO_PATH)\n",
    "print(\"Our class names: {}\".format(class_names,))\n",
    "\n",
    "audio_paths = []\n",
    "labels = []\n",
    "for label, name in enumerate(class_names):\n",
    "    label = int(name)\n",
    "    print(\"Processing speaker {}\".format(name,))\n",
    "    print(\"Actual Label \",label)\n",
    "    dir_path = Path(DATASET_AUDIO_PATH) / name\n",
    "    speaker_sample_paths = [\n",
    "        os.path.join(dir_path, filepath)\n",
    "        for filepath in os.listdir(dir_path)\n",
    "        if filepath.endswith(\".wav\")\n",
    "    ]\n",
    "    audio_paths += speaker_sample_paths\n",
    "    labels += [label] * len(speaker_sample_paths)\n",
    "\n",
    "print(\n",
    "    \"Found {} files belonging to {} classes.\".format(len(audio_paths), len(class_names))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2dc4a6",
   "metadata": {
    "papermill": {
     "duration": 0.036496,
     "end_time": "2022-03-29T12:20:25.012935",
     "exception": false,
     "start_time": "2022-03-29T12:20:24.976439",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## A. Adding External Data ( 150 ASVspoof + 150 LibriSpeech)\n",
    "\n",
    "As we are using supervised learning algorithms for our solution, the unknown class needs to be as much diversified as possible. In order to achieve that we are using the given 6000 data in 6 class including the unknown class. Along with it we are also using random 150 data from librispeech dataset and 150 data from asvspoof dataset to introduce variation in the unknown set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb2915a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:20:25.091478Z",
     "iopub.status.busy": "2022-03-29T12:20:25.090924Z",
     "iopub.status.idle": "2022-03-29T12:20:25.722353Z",
     "shell.execute_reply": "2022-03-29T12:20:25.723275Z",
     "shell.execute_reply.started": "2022-03-29T09:23:13.968847Z"
    },
    "papermill": {
     "duration": 0.674279,
     "end_time": "2022-03-29T12:20:25.723471",
     "exception": false,
     "start_time": "2022-03-29T12:20:25.049192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30000 files\n",
      "File taken from ASVSPOOF: 150\n",
      "Total Number of sample in dataset : 6150\n"
     ]
    }
   ],
   "source": [
    "## Adding ASVSpoof data\n",
    "## run this only once\n",
    "# our desired sample size after this step will be 6150\n",
    "\n",
    "eval_path = ASVSPOOF_DATA_PATH \n",
    "speaker_sample_paths = [ os.path.join(eval_path, filepath) \n",
    "                        for filepath in os.listdir(eval_path) if filepath.endswith(\".wav\") ] \n",
    "X_asv = [] \n",
    "X_asv += speaker_sample_paths\n",
    "X_asv = X_asv[:30000]\n",
    "label_asv = [5]*len(X_asv)\n",
    "print( \"Found {} files\".format(len(X_asv)))\n",
    "check, X_taken, _, _ = train_test_split(X_asv, label_asv, test_size=0.005, random_state=seed)\n",
    "print(\"File taken from ASVSPOOF:\",len(X_taken))\n",
    "if len(audio_paths)<6150:\n",
    "    audio_paths+= X_taken\n",
    "    labels += [5]*len(X_taken)\n",
    "print(\"Total Number of sample in dataset :\",len(audio_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c14f0c",
   "metadata": {
    "papermill": {
     "duration": 0.036487,
     "end_time": "2022-03-29T12:20:25.796921",
     "exception": false,
     "start_time": "2022-03-29T12:20:25.760434",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Another 150 sample of Librispeech Data is added to get more variety in the unknown data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79f83658",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:20:25.875907Z",
     "iopub.status.busy": "2022-03-29T12:20:25.875335Z",
     "iopub.status.idle": "2022-03-29T12:20:26.036762Z",
     "shell.execute_reply": "2022-03-29T12:20:26.036295Z",
     "shell.execute_reply.started": "2022-03-29T09:23:14.710958Z"
    },
    "papermill": {
     "duration": 0.203492,
     "end_time": "2022-03-29T12:20:26.036874",
     "exception": false,
     "start_time": "2022-03-29T12:20:25.833382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 files\n",
      "File taken from LibriSpeech: 150\n",
      "Total Number of sample in dataset : 6300\n"
     ]
    }
   ],
   "source": [
    "## Adding Librispeech data\n",
    "## another 150 sample is added into class 5\n",
    "\n",
    "eval_path = LIBRISPEECH_DATA_PATH \n",
    "speaker_sample_paths = [ os.path.join(eval_path, filepath) \n",
    "                        for filepath in os.listdir(eval_path) if filepath.endswith(\".wav\") ] \n",
    "X_asv = [] \n",
    "X_asv += speaker_sample_paths\n",
    "label_asv = [5]*len(X_asv)\n",
    "print( \"Found {} files\".format(len(X_asv)))\n",
    "X_taken,libri_test, _, _ = train_test_split(X_asv, label_asv, test_size=0.85, random_state=seed)\n",
    "print(\"File taken from LibriSpeech:\",len(X_taken))\n",
    "if len(audio_paths)<6300:\n",
    "    audio_paths+= X_taken\n",
    "    labels += [5]*len(X_taken)\n",
    "print(\"Total Number of sample in dataset :\",len(audio_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86099383",
   "metadata": {
    "papermill": {
     "duration": 0.036048,
     "end_time": "2022-03-29T12:20:26.109892",
     "exception": false,
     "start_time": "2022-03-29T12:20:26.073844",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now as our extra data addition is done, we are splitting the dataset into train and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa35087f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:20:26.187261Z",
     "iopub.status.busy": "2022-03-29T12:20:26.186549Z",
     "iopub.status.idle": "2022-03-29T12:20:26.192370Z",
     "shell.execute_reply": "2022-03-29T12:20:26.192812Z",
     "shell.execute_reply.started": "2022-03-29T09:23:14.940485Z"
    },
    "papermill": {
     "duration": 0.046711,
     "end_time": "2022-03-29T12:20:26.192933",
     "exception": false,
     "start_time": "2022-03-29T12:20:26.146222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(audio_paths, labels, test_size=TRAIN_TEST_SPLIT, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9ac5f78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:20:26.280054Z",
     "iopub.status.busy": "2022-03-29T12:20:26.279238Z",
     "iopub.status.idle": "2022-03-29T12:20:26.282804Z",
     "shell.execute_reply": "2022-03-29T12:20:26.282342Z",
     "shell.execute_reply.started": "2022-03-29T09:23:14.950408Z"
    },
    "papermill": {
     "duration": 0.044015,
     "end_time": "2022-03-29T12:20:26.282908",
     "exception": false,
     "start_time": "2022-03-29T12:20:26.238893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples in train dataset:  4410\n",
      "Samples in validation dataset: 1890\n"
     ]
    }
   ],
   "source": [
    "#checking if the size is alright\n",
    "print(\"Samples in train dataset: \",len(X_train))\n",
    "print(\"Samples in validation dataset:\",len(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf01357d",
   "metadata": {
    "papermill": {
     "duration": 0.036794,
     "end_time": "2022-03-29T12:20:26.356467",
     "exception": false,
     "start_time": "2022-03-29T12:20:26.319673",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## B. Creating Repeated Dataset\n",
    "\n",
    "Although cropping dataset to the minimum length or less could help us uniform the dataset, our data variance is very high(approximately 2.5 to 14.5s). In order to avoid such data loss we took sample size to be 10 sec and repeated the samples shorter than that instead of zero padding or other methods as in several ASR research it has shown better performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aba87b2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:20:26.437718Z",
     "iopub.status.busy": "2022-03-29T12:20:26.436998Z",
     "iopub.status.idle": "2022-03-29T12:20:26.439615Z",
     "shell.execute_reply": "2022-03-29T12:20:26.439215Z",
     "shell.execute_reply.started": "2022-03-29T09:23:14.960341Z"
    },
    "papermill": {
     "duration": 0.045835,
     "end_time": "2022-03-29T12:20:26.439718",
     "exception": false,
     "start_time": "2022-03-29T12:20:26.393883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# utility functions for repeating audio files\n",
    "def repeated_data(file_path):\n",
    "    \"\"\" This function will take a file path and give out truncated and padded to 10s version waveform\"\"\"\n",
    "    y, sr = librosa.load(file_path,sr=SR)\n",
    "    aud_length = AUD_LENGTH*sr # making all audio length 10 s and truncating the rest\n",
    "    duration = librosa.get_duration(y=y, sr=sr)\n",
    "    if duration < AUD_LENGTH:\n",
    "        y = np.tile(y, int((aud_length/sr) // duration)+1)\n",
    "    y = librosa.resample(y[:aud_length], orig_sr=sr, target_sr=SR)\n",
    "    return y\n",
    "\n",
    "def repeated_dataset(dataset):\n",
    "    \"\"\" This function generated waveshape dataset\"\"\"\n",
    "    new_ds = []\n",
    "    for f in dataset:\n",
    "        new_ds.append(repeated_data(f))\n",
    "    return new_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bff643",
   "metadata": {
    "papermill": {
     "duration": 0.037647,
     "end_time": "2022-03-29T12:20:26.514077",
     "exception": false,
     "start_time": "2022-03-29T12:20:26.476430",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## C. Creating DataGenerator\n",
    "\n",
    "As we were facing memory issue with saving such huge array of raw waveshape dataset and ran out of memory for quite a period, we used this datagenerator in order to efficiently load the model into our training system using the allowed resources.\n",
    "\n",
    "Also, we are using tf dataset API for processing our dataset as it gives better performance while using Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bab8709c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:20:26.590780Z",
     "iopub.status.busy": "2022-03-29T12:20:26.589990Z",
     "iopub.status.idle": "2022-03-29T12:20:26.602430Z",
     "shell.execute_reply": "2022-03-29T12:20:26.601979Z",
     "shell.execute_reply.started": "2022-03-29T09:23:14.973832Z"
    },
    "papermill": {
     "duration": 0.051759,
     "end_time": "2022-03-29T12:20:26.602537",
     "exception": false,
     "start_time": "2022-03-29T12:20:26.550778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size= BATCH_SIZE, \n",
    "                 n_classes=6, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = AUD_LENGTH * SR\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.shuffle = shuffle\n",
    "        self.list_IDs = list_IDs\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def path_to_audio(self,path):\n",
    "        \"\"\"Reads and decodes an audio file.\"\"\"\n",
    "        return repeated_data(path)\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples'\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            _tempx = self.path_to_audio(self.list_IDs[ID])\n",
    "            #_tempx = self.spect_audio(_tempx)\n",
    "            X.append(_tempx)\n",
    "\n",
    "            # Store class\n",
    "            y.append(self.labels[ID])\n",
    "\n",
    "        return np.reshape(np.array(X), (self.batch_size,SR*AUD_LENGTH,1)).astype(np.float32),np.array(y).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf34eda3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:20:26.680236Z",
     "iopub.status.busy": "2022-03-29T12:20:26.679430Z",
     "iopub.status.idle": "2022-03-29T12:20:26.681345Z",
     "shell.execute_reply": "2022-03-29T12:20:26.681722Z",
     "shell.execute_reply.started": "2022-03-29T09:23:14.990371Z"
    },
    "papermill": {
     "duration": 0.042621,
     "end_time": "2022-03-29T12:20:26.681841",
     "exception": false,
     "start_time": "2022-03-29T12:20:26.639220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generating tf datasets\n",
    "train_ds = DataGenerator(X_train,y_train)\n",
    "valid_ds = DataGenerator(X_val,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac76f429",
   "metadata": {
    "papermill": {
     "duration": 0.036625,
     "end_time": "2022-03-29T12:20:26.755345",
     "exception": false,
     "start_time": "2022-03-29T12:20:26.718720",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Building A Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de1f912",
   "metadata": {
    "papermill": {
     "duration": 0.036937,
     "end_time": "2022-03-29T12:20:26.829367",
     "exception": false,
     "start_time": "2022-03-29T12:20:26.792430",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## A. Resnet based model\n",
    "\n",
    "For our model we have tried out different models and fine tuned them for the best performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dae65b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:20:26.919366Z",
     "iopub.status.busy": "2022-03-29T12:20:26.918792Z",
     "iopub.status.idle": "2022-03-29T12:20:29.810448Z",
     "shell.execute_reply": "2022-03-29T12:20:29.810963Z",
     "shell.execute_reply.started": "2022-03-29T09:23:14.999412Z"
    },
    "papermill": {
     "duration": 2.944815,
     "end_time": "2022-03-29T12:20:29.811134",
     "exception": false,
     "start_time": "2022-03-29T12:20:26.866319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 160000, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 160000, 16)   64          ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 160000, 16)  64          ['conv1d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 160000, 16)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 40000, 16)    0           ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 40000, 32)    1568        ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 40000, 32)   128         ['conv1d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 40000, 32)    0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 40000, 32)    3104        ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 40000, 32)   128         ['conv1d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 40000, 32)    0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 40000, 32)    544         ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 40000, 32)    3104        ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 80000, 32)    0           ['conv1d_4[0][0]',               \n",
      "                                                                  'conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 80000, 32)    0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 20000, 32)   0           ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 20000, 64)    6208        ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 20000, 64)   256         ['conv1d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 20000, 64)    0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 20000, 64)    12352       ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 20000, 64)   256         ['conv1d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 20000, 64)    0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 20000, 64)    2112        ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 20000, 64)    12352       ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 40000, 64)    0           ['conv1d_8[0][0]',               \n",
      "                                                                  'conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 40000, 64)    0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 10000, 64)   0           ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 10000, 128)   24704       ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 10000, 128)  512         ['conv1d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 10000, 128)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)             (None, 10000, 128)   49280       ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 10000, 128)  512         ['conv1d_10[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_8 (ReLU)                 (None, 10000, 128)   0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)             (None, 10000, 128)   8320        ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)             (None, 10000, 128)   49280       ['re_lu_8[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 20000, 128)   0           ['conv1d_12[0][0]',              \n",
      "                                                                  'conv1d_11[0][0]']              \n",
      "                                                                                                  \n",
      " re_lu_9 (ReLU)                 (None, 20000, 128)   0           ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 5000, 128)   0           ['re_lu_9[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)             (None, 5000, 128)    49280       ['max_pooling1d_3[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 5000, 128)   512         ['conv1d_13[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_10 (ReLU)                (None, 5000, 128)    0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)             (None, 5000, 128)    49280       ['re_lu_10[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 5000, 128)   512         ['conv1d_14[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_11 (ReLU)                (None, 5000, 128)    0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)             (None, 5000, 128)    16512       ['max_pooling1d_3[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)             (None, 5000, 128)    49280       ['re_lu_11[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 10000, 128)   0           ['conv1d_16[0][0]',              \n",
      "                                                                  'conv1d_15[0][0]']              \n",
      "                                                                                                  \n",
      " re_lu_12 (ReLU)                (None, 10000, 128)   0           ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 78, 128)     0           ['re_lu_12[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 9984)         0           ['max_pooling1d_4[0][0]']        \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           639040      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           2080        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 6)            198         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 981,542\n",
      "Trainable params: 980,102\n",
      "Non-trainable params: 1,440\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Model Name == \"resnet_version\"\n",
    "\n",
    "# Resnet Block\n",
    "def residual_block(xx, filters):\n",
    "    \"\"\" This Block will work as the repeating Resnet Block for extracting important features from the waveshape\"\"\"\n",
    "    yy = tf.keras.layers.Conv1D(filters, kernel_size = 3, padding=\"same\")(xx)\n",
    "    yy = tf.keras.layers.BatchNormalization()(yy)\n",
    "    yy = tf.keras.layers.ReLU()(yy)\n",
    "    \n",
    "    yy = tf.keras.layers.Conv1D(filters, kernel_size = 3, padding=\"same\")(yy)\n",
    "    yy = tf.keras.layers.BatchNormalization()(yy)\n",
    "    yy = tf.keras.layers.ReLU()(yy)\n",
    "    \n",
    "    yy = tf.keras.layers.Conv1D(filters, kernel_size = 3, padding=\"same\")(yy)\n",
    "    \n",
    "    xx = tf.keras.layers.Conv1D(filters, kernel_size = 1, padding=\"same\")(xx)\n",
    "    \n",
    "    xx = tf.keras.layers.Concatenate(axis=1)([xx,yy])\n",
    "    xx = tf.keras.layers.ReLU()(xx)\n",
    "    \n",
    "    return xx\n",
    "\n",
    "def resnet_version(input_shape, num_classes):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, name=\"input\")\n",
    "    x      = tf.keras.layers.Conv1D(16, kernel_size = 3, padding=\"same\")(inputs)\n",
    "    x      = tf.keras.layers.BatchNormalization()(x)\n",
    "    x      = tf.keras.layers.ReLU()(x)\n",
    "    x      = tf.keras.layers.MaxPool1D(pool_size = 4)(x)\n",
    "    \n",
    "    # stacked resnet modules\n",
    "    # res1\n",
    "    x      = residual_block(x,32)\n",
    "    x      = tf.keras.layers.MaxPool1D(pool_size = 4)(x)\n",
    "    # res2\n",
    "    x      = residual_block(x,64)\n",
    "    x      = tf.keras.layers.MaxPool1D(pool_size = 4)(x)\n",
    "    # res3\n",
    "    x      = residual_block(x,128)\n",
    "    x      = tf.keras.layers.MaxPool1D(pool_size = 4)(x)\n",
    "    # res4\n",
    "    x      = residual_block(x,128)\n",
    "    x      = tf.keras.layers.MaxPool1D(pool_size = x.shape[-1])(x)\n",
    "    \n",
    "    x      = tf.keras.layers.Flatten()(x)\n",
    "    x      = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "    x      = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\", name=\"output\")(x)\n",
    "    \n",
    "    return tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "aud_length = AUD_LENGTH * SR\n",
    "\n",
    "model = resnet_version((aud_length, 1), len(class_names))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e570d644",
   "metadata": {
    "papermill": {
     "duration": 0.038375,
     "end_time": "2022-03-29T12:20:29.904517",
     "exception": false,
     "start_time": "2022-03-29T12:20:29.866142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Training the Model\n",
    "\n",
    "for training we have chosed the cross validation method as it is proven to work well as the model can learn different distribution from diff classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5286fca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:20:29.988066Z",
     "iopub.status.busy": "2022-03-29T12:20:29.986506Z",
     "iopub.status.idle": "2022-03-29T12:20:29.988675Z",
     "shell.execute_reply": "2022-03-29T12:20:29.989066Z",
     "shell.execute_reply.started": "2022-03-29T09:23:17.608697Z"
    },
    "papermill": {
     "duration": 0.046155,
     "end_time": "2022-03-29T12:20:29.989218",
     "exception": false,
     "start_time": "2022-03-29T12:20:29.943063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "EPOCHS=1\n",
    "NFOLDS=5\n",
    "\n",
    "# chossing the model parameters\n",
    "MODEL = \"resnet_version\"\n",
    "LOSS = \"sparse_categorical_crossentropy\"\n",
    "OPTIMIZER = \"Adam\"\n",
    "\n",
    "# setting model and weight name\n",
    "MODEL_NAME = \"model_resnet1D_cv_\"\n",
    "WEIGHT_NAME = \"weight_resnet1D_cv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "876b6e31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:20:30.081947Z",
     "iopub.status.busy": "2022-03-29T12:20:30.081121Z",
     "iopub.status.idle": "2022-03-29T12:27:15.745131Z",
     "shell.execute_reply": "2022-03-29T12:27:15.745736Z",
     "shell.execute_reply.started": "2022-03-29T09:23:17.615967Z"
    },
    "papermill": {
     "duration": 405.717039,
     "end_time": "2022-03-29T12:27:15.745942",
     "exception": false,
     "start_time": "2022-03-29T12:20:30.028903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- > Fold 1 < ---------------\n",
      "504/504 [==============================] - 310s 518ms/step - loss: 1.0128 - accuracy: 0.5869 - val_loss: 3.0593 - val_accuracy: 0.4630 - lr: 0.0010\n",
      "189/189 [==============================] - 5s 27ms/step - loss: 3.0593 - accuracy: 0.4630\n",
      ">0.463\n",
      "--------------- > Fold 2 < ---------------\n",
      "504/504 [==============================] - 68s 130ms/step - loss: 0.8000 - accuracy: 0.7065 - val_loss: 1.0098 - val_accuracy: 0.6201 - lr: 0.0010\n",
      "189/189 [==============================] - 5s 27ms/step - loss: 1.0098 - accuracy: 0.6201\n",
      ">0.620\n",
      "--------------- > Fold 3 < ---------------\n",
      "504/504 [==============================] - 50s 96ms/step - loss: 1.0593 - accuracy: 0.5647 - val_loss: 1.6502 - val_accuracy: 0.5111 - lr: 0.0010\n",
      "189/189 [==============================] - 5s 27ms/step - loss: 1.6502 - accuracy: 0.5111\n",
      ">0.511\n",
      "--------------- > Fold 4 < ---------------\n",
      "504/504 [==============================] - 50s 96ms/step - loss: 1.0276 - accuracy: 0.5661 - val_loss: 1.4292 - val_accuracy: 0.5370 - lr: 0.0010\n",
      "189/189 [==============================] - 5s 27ms/step - loss: 1.4292 - accuracy: 0.5370\n",
      ">0.537\n",
      "--------------- > Fold 5 < ---------------\n",
      "504/504 [==============================] - 50s 96ms/step - loss: 0.7797 - accuracy: 0.6619 - val_loss: 1.2517 - val_accuracy: 0.5037 - lr: 0.0010\n",
      "189/189 [==============================] - 5s 27ms/step - loss: 1.2517 - accuracy: 0.5037\n",
      ">0.504\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=NFOLDS)\n",
    "splits = folds.split(audio_paths, labels)\n",
    "\n",
    "def evaluate_model(X_train, X_val, y_train, y_val,j):\n",
    "    \n",
    "    train_ds = DataGenerator(X_train,y_train)\n",
    "    valid_ds = DataGenerator(X_val,y_val)\n",
    "    \n",
    "    aud_length = AUD_LENGTH * SR\n",
    "    \n",
    "    if MODEL == \"resnet_version\":\n",
    "        model = resnet_version((aud_length, 1), len(class_names))\n",
    "        \n",
    "    epochs = EPOCHS\n",
    "    batch_size = BATCH_SIZE\n",
    "    \n",
    "    model.compile(\n",
    "    optimizer= OPTIMIZER, loss= LOSS, metrics=[\"accuracy\"])\n",
    "    weight_save_filename = WEIGHT_NAME +str(j)+\"fold_.h5\"\n",
    "    \n",
    "    lr_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=1, mode='min', min_lr=1e-9)\n",
    "    earlystopping_cb = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.001, patience=10, mode='min', restore_best_weights=True)\n",
    "    mdlcheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(weight_save_filename, monitor=\"val_accuracy\", save_best_only=True,save_weights_only=True)\n",
    "    \n",
    "    history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_ds,\n",
    "    callbacks=[lr_reduce,earlystopping_cb, mdlcheckpoint_cb],\n",
    ")\n",
    " \n",
    "    _, val_acc = model.evaluate(valid_ds, verbose = 1)\n",
    "    \n",
    "    model.load_weights(os.path.join(WEIGHT_SAVE_DIR,weight_save_filename)) #\n",
    "    model.save(os.path.join(MODEL_SAVE_DIR,MODEL_NAME + str(j)+\"fold_.h5\"))\n",
    "    return model, val_acc\n",
    "\n",
    "fin_model = 1\n",
    "cv_scores, model_history = list(), list()\n",
    "train = audio_paths\n",
    "targets = labels\n",
    "for fold, (train_idx, val_idx) in enumerate(splits):\n",
    "    X_train = []\n",
    "    X_valid = []\n",
    "    y_train = []\n",
    "    y_valid = []\n",
    "    for i in train_idx:\n",
    "        X_train.append(train[i])\n",
    "        y_train.append(targets[i])\n",
    "    for j in val_idx:\n",
    "        X_valid.append(train[j])\n",
    "        y_valid.append(targets[j])\n",
    "\n",
    "    print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n",
    "    model, val_acc = evaluate_model(X_train, X_val, y_train, y_val,fold)\n",
    "    print('>%.3f' % val_acc)\n",
    "    cv_scores.append(val_acc)\n",
    "    if val_acc == max(cv_scores):\n",
    "        fin_model = model\n",
    "    model_history.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381b2f6b",
   "metadata": {
    "papermill": {
     "duration": 0.297313,
     "end_time": "2022-03-29T12:27:16.303604",
     "exception": false,
     "start_time": "2022-03-29T12:27:16.006291",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generating Predictions\n",
    "\n",
    "we will be using the ensemble of each model generated by our cross validation method to get the least variance in our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f28e283",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:27:17.034026Z",
     "iopub.status.busy": "2022-03-29T12:27:17.033133Z",
     "iopub.status.idle": "2022-03-29T12:27:17.034836Z",
     "shell.execute_reply": "2022-03-29T12:27:17.035351Z",
     "shell.execute_reply.started": "2022-03-29T09:30:03.613206Z"
    },
    "papermill": {
     "duration": 0.306489,
     "end_time": "2022-03-29T12:27:17.035490",
     "exception": false,
     "start_time": "2022-03-29T12:27:16.729001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ensemble_predictions(members, testX,testy=1):\n",
    "    yhats = [model.predict(testX) for model in members]\n",
    "    yhats = np.array(yhats)\n",
    "    # sum across ensemble members\n",
    "    summed = np.sum(yhats, axis=0)\n",
    "    # argmax across classes\n",
    "    result = np.argmax(summed, axis=1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "326656df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T12:27:17.550448Z",
     "iopub.status.busy": "2022-03-29T12:27:17.549608Z",
     "iopub.status.idle": "2022-03-29T12:27:56.874835Z",
     "shell.execute_reply": "2022-03-29T12:27:56.875328Z",
     "shell.execute_reply.started": "2022-03-29T09:30:03.620421Z"
    },
    "papermill": {
     "duration": 39.585351,
     "end_time": "2022-03-29T12:27:56.875488",
     "exception": false,
     "start_time": "2022-03-29T12:27:17.290137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, ..., 5, 4, 5], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = ensemble_predictions(model_history, valid_ds)\n",
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 900.846044,
   "end_time": "2022-03-29T12:35:09.484950",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-29T12:20:08.638906",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
